# E-commerce Data Engineering Pipeline

A complete end-to-end mini data-engineering system for an e-commerce dataset with production-grade quality.

## ğŸŒŸ Overview

This project provides a comprehensive data engineering pipeline that:

- Generates synthetic e-commerce datasets with controlled randomness
- Defines a clean, normalized relational data model
- Implements an ETL pipeline into SQLite with validation
- Includes data validation using Pandera schemas
- Provides automated data profiling reports using ydata-profiling
- Includes data validation, logging, tests, and auto-generated documentation
- Provides SQL analysis queries for business insights
- Offers REST API access using FastAPI
- Includes interactive analytics dashboard using Streamlit
- Is ready for GitHub with CI pipeline and comprehensive documentation
- Supports containerization with Docker

## ğŸ“Š Entity Relationship Diagram

```
customers â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚             â”‚
            â–¼             â–¼
          orders â”€â”€â”¬â”€â”€â”€â–º payments
                   â”‚
                   â–¼
              order_items â”€â”€â–º products
```

## ğŸ“ Project Structure

```
ecom_project/
â”œâ”€â”€ api/               # REST API using FastAPI
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ dashboard/         # Streamlit analytics dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ data/              # Generated CSV files
â”œâ”€â”€ database/          # SQLite database
â”œâ”€â”€ docs/              # Auto-generated documentation
â”‚   â””â”€â”€ profiling_reports/  # Data profiling reports
â”œâ”€â”€ etl/               # ETL pipeline scripts
â”‚   â”œâ”€â”€ generate_data.py
â”‚   â””â”€â”€ load_sqlite.py
â”œâ”€â”€ models/            # Data models and schema
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ scripts/           # Utility scripts
â”‚   â”œâ”€â”€ generate_docs.py
â”‚   â”œâ”€â”€ generate_profiles.py
â”‚   â””â”€â”€ run_all.py
â”œâ”€â”€ sql/               # SQL analysis queries
â”‚   â””â”€â”€ analysis.sql
â”œâ”€â”€ tests/             # Test suite
â”‚   â”œâ”€â”€ test_data_generation.py
â”‚   â””â”€â”€ test_etl.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ docker-compose.yml # Service orchestration
â”œâ”€â”€ Makefile           # Build automation
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### 1. Setup

```bash
# Clone the repository
git clone <repository-url>
cd ecom_project

# Install dependencies
pip install -r requirements.txt
```

### 2. Using Makefile (Recommended)

The project includes a Makefile for streamlined operations:

```bash
# Install dependencies
make setup

# Generate data
make generate

# Ingest data into database
make ingest

# Run complete pipeline
make run

# Generate profiling reports
make profile

# Generate documentation
make docs

# Run tests
make test

# Start REST API
make api

# Start Streamlit dashboard
make dashboard

# Clean generated files
make clean
```

### 2. Run the Complete Pipeline

```bash
# Run the entire pipeline with one command
python scripts/run_all.py
```

This will:
1. Generate synthetic data (2,000 customers, 600 products, 4,000 orders, etc.)
2. Load data into SQLite database
3. Run SQL analysis queries
4. Generate documentation

### 3. Individual Steps

#### Generate Data
```bash
cd etl
python generate_data.py
```

#### Load into SQLite
```bash
cd etl
python load_sqlite.py
```

#### Run Analysis Queries
```bash
# Use any SQLite client to run queries from sql/analysis.sql
# Or examine the file directly
cat sql/analysis.sql
```

#### Generate Documentation
```bash
cd scripts
python generate_docs.py
```

### 4. Using Docker

The project supports containerization with Docker:

```bash
# Build the Docker image
make docker-build
# or
docker-compose build

# Run the complete pipeline
make docker-run
# or
docker-compose up pipeline

# Start all services (pipeline, API, dashboard)
docker-compose up

# Start specific services
docker-compose up api         # REST API on port 8000
docker-compose up dashboard   # Streamlit dashboard on port 8501
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Install pytest if not already installed
pip install pytest

# Run all tests
pytest tests/ -v
```

## ğŸ“ˆ SQL Analysis Queries

The project includes several analytical SQL queries in `sql/analysis.sql`:

1. **Top 50 customers by spend**
2. **Top products by revenue**
3. **Revenue by category**
4. **Revenue time series** (daily, weekly, monthly)
5. **Average order value per customer**
6. **Fraud-like anomalies** (orders with mismatched payments)
7. **Power user analysis** (customers with > 90th percentile order volume)

## ğŸŒ REST API

The project includes a FastAPI-based REST API for programmatic access to the data:

- **Customers**: `/customers/`, `/customers/{id}`
- **Products**: `/products/`, `/products/{id}`
- **Orders**: `/orders/`, `/orders/{id}`
- **Analytics**: `/analytics/revenue/daily`, `/analytics/top-customers`, `/analytics/top-products`

Start the API with `make api` or `python api/main.py`, then access at `http://localhost:8000`.

## ğŸ“Š Streamlit Dashboard

The project includes an interactive analytics dashboard built with Streamlit:

- Overview of system metrics
- Customer analytics
- Product analytics
- Order analytics
- Business intelligence visualizations

Start the dashboard with `make dashboard` or `streamlit run dashboard/app.py`, then access at `http://localhost:8501`.

## ğŸ› ï¸ Data Model

The database schema is fully normalized to 3NF with:

- **Customers**: customer information
- **Products**: product catalog
- **Orders**: order records
- **Order Items**: individual items within orders
- **Payments**: payment records

All tables have appropriate constraints, foreign keys, and indexes for performance.

## ğŸ“š Documentation

Auto-generated documentation is available in `docs/data_dictionary.md` after running the pipeline.

## ğŸ”„ CI/CD Pipeline

GitHub Actions workflow includes:

- Dependency installation
- Code linting with flake8
- SQL syntax validation
- Test execution
- Artifact upload

## ğŸ“¦ Requirements

- Python 3.7+
- See `requirements.txt` for Python package dependencies

New dependencies for enhanced features:
- **ydata-profiling**: For automated data profiling reports
- **pandera**: For data schema validation
- **fastapi**: For REST API functionality
- **uvicorn**: For API server
- **streamlit**: For interactive dashboard
- **plotly**: For dashboard visualizations

## ğŸ“¤ Push to GitHub

```bash
# Initialize git repository (if not already done)
git init

# Add all files
git add .

# Commit changes
git commit -m "Initial commit: E-commerce data engineering pipeline"

# Add remote origin (replace with your repository URL)
git remote add origin https://github.com/your-username/ecom-project.git

# Push to GitHub
git push -u origin main
```"# Ecom-synthetic-data-platform" 
